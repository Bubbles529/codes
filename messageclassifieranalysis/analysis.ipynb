{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import loaddata\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import sklearn.feature_extraction.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages = loaddata.load_message()\n",
    "content = np.array([m[0] for m in messages])\n",
    "target = np.array([m[1] for m in messages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.845 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.845 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23734, 21959)\n"
     ]
    }
   ],
   "source": [
    "class MessageCountVectorizer(sklearn.feature_extraction.text.CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        def analyzer(doc):\n",
    "            words = pseg.cut(doc)\n",
    "            new_doc = ''.join(w.word for w in words if w.flag != 'x')\n",
    "            words = jieba.cut(new_doc)\n",
    "            return words\n",
    "        return analyzer\n",
    "\n",
    "vec_count = MessageCountVectorizer(min_df=2,max_df=0.8)\n",
    "data_count = vec_count.fit_transform(content)\n",
    "vec_count.get_feature_names()\n",
    "print(data_count.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.96089997471981126, SVC(C=0.5, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False), {'C': 0.5, 'kernel': 'linear'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "def find_best_svm(data, target, cv):\n",
    "    clf = SVC()\n",
    "    C = [0.1, 0.5, 0.75, 1, 2, 3, 5, 10, 20]\n",
    "    kernel = ['linear']#, 'poly', 'rbf']\n",
    "    param_grid = [{'C': C, 'kernel':kernel}]\n",
    "    grid_search = GridSearchCV(clf, param_grid=param_grid, cv=cv)\n",
    "    grid_search.fit(data, target)\n",
    "    grid_search.cls_name = 'SVM'\n",
    "    return grid_search\n",
    "grid_svm = find_best_svm(data_count, target, cv=10)\n",
    "grid_svm.grid_scores_\n",
    "grid_svm.best_score_, grid_svm.best_estimator_, grid_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.963774220725\n",
      "1 0.965459140691\n",
      "2 0.963774220725\n",
      "3 0.96798652064\n",
      "4 0.973041280539\n",
      "5 0.961668070767\n",
      "6 0.958719460826\n",
      "7 0.964616680708\n",
      "8 0.960825610783\n",
      "9 0.965037910699\n",
      "|class|3|2|6|1|4|5|\n",
      "|-|\n",
      "|0|0.6667|0.9614|0.8689|0.9915|0.9062|0.9563|\n",
      "|1|0.7647|0.9629|0.8833|0.9857|0.9126|0.9588|\n",
      "|2|0.5789|0.9590|0.8621|0.9954|0.9245|0.9433|\n",
      "|3|0.7500|0.9595|0.8806|0.9963|0.9091|0.9619|\n",
      "|4|0.7619|0.9686|0.9130|0.9934|0.9202|0.9713|\n",
      "|5|0.7000|0.9623|0.8421|0.9937|0.8579|0.9472|\n",
      "|6|0.6429|0.9610|0.8734|0.9912|0.8557|0.9536|\n",
      "|7|0.6364|0.9671|0.7619|0.9911|0.9222|0.9500|\n",
      "|8|0.7333|0.9531|0.7941|0.9908|0.9029|0.9568|\n",
      "|9|0.6429|0.9696|0.9362|0.9935|0.8929|0.9421|\n",
      "|max|0.7647|0.9696|0.9362|0.9963|0.9245|0.9713|\n",
      "|min|0.5789|0.9531|0.7619|0.9857|0.8557|0.9421|\n",
      "|mean|0.6878|0.9624|0.8616|0.9923|0.9004|0.9541|\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "def get_classes_accury(data, target, test_times = 10, test_size=0.1):\n",
    "    scores = np.zeros((test_times,len(set(target))))\n",
    "    for t in range(test_times):\n",
    "        clf = SVC(C=0.5, cache_size=200, class_weight=None, coef0=0.0,\n",
    "   decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
    "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "   tol=0.001, verbose=False)\n",
    "        Xtrain, Xtest, ytrain, ytest = train_test_split(data, target, test_size=test_size,\n",
    "                                                    random_state=t)\n",
    "        clf.fit(Xtrain, ytrain)\n",
    "        print(t, clf.score(Xtest, ytest))\n",
    "        pre = clf.predict(Xtest)\n",
    "        for i,c in enumerate(list(set(target))):\n",
    "            s = np.logical_and(pre==c, ytest==c).sum()/ (ytest==c).sum()\n",
    "            scores[t, i] = s\n",
    "\n",
    "    ##### 生成表格\n",
    "    print('|'+'class'+'|'+'|'.join([str(i) for i  in list(set(target))])+'|')\n",
    "    print('|'+'-'+'|')\n",
    "    for i,score in enumerate(scores):\n",
    "        print( '|'+str(i)+'|'+ '|'.join(['{:.4f}'.format(_) for _ in score])+ '|' )\n",
    "    print( '|'+'max'+ '|'+ '|'.join(['{:.4f}'.format(_) for _ in scores.max(axis=0)])+ '|' )\n",
    "    print( '|'+'min'+ '|'+ '|'.join(['{:.4f}'.format(_) for _ in scores.min(axis=0)])+ '|' )\n",
    "    print( '|'+'mean'+'|'+  '|'.join(['{:.4f}'.format(_) for _ in scores.mean(axis=0)])+ '|' )\n",
    "\n",
    "    return scores\n",
    "scores = get_classes_accury(data_count, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23734, 21959)\n"
     ]
    }
   ],
   "source": [
    "#直接采用TFID生成对应的\n",
    "class TfidfVectorizer(sklearn.feature_extraction.text.TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        #analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        def analyzer(doc):\n",
    "            words = pseg.cut(doc)\n",
    "            new_doc = ''.join(w.word for w in words if w.flag != 'x')\n",
    "            words = jieba.cut(new_doc)\n",
    "            return words\n",
    "        return analyzer\n",
    "\n",
    "vec_tfidf = TfidfVectorizer(min_df=2,max_df=0.8)\n",
    "data_tfidf = vec_tfidf.fit_transform(content)\n",
    "print(data_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.91278, std: 0.02150, params: {'C': 0.1, 'kernel': 'linear'}, mean: 0.95285, std: 0.01301, params: {'C': 0.5, 'kernel': 'linear'}, mean: 0.95761, std: 0.01146, params: {'C': 0.75, 'kernel': 'linear'}, mean: 0.96023, std: 0.01070, params: {'C': 1, 'kernel': 'linear'}, mean: 0.96292, std: 0.01068, params: {'C': 2, 'kernel': 'linear'}, mean: 0.96259, std: 0.01109, params: {'C': 3, 'kernel': 'linear'}, mean: 0.96263, std: 0.01101, params: {'C': 4, 'kernel': 'linear'}, mean: 0.96212, std: 0.01083, params: {'C': 5, 'kernel': 'linear'}, mean: 0.96136, std: 0.01085, params: {'C': 10, 'kernel': 'linear'}, mean: 0.96010, std: 0.01074, params: {'C': 30, 'kernel': 'linear'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9629223898205107, SVC(C=2, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False), {'C': 2, 'kernel': 'linear'})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_SVM(data, target):\n",
    "    from sklearn.grid_search import GridSearchCV\n",
    "    from sklearn.svm import SVC\n",
    "    clf = SVC()\n",
    "\n",
    "    C = [0.1, 0.5, 0.75, 1, 2, 3, 4, 5, 10, 30]\n",
    "    kernel = ['linear'] # 'poly',\n",
    "    param_grid = [{'C': C, 'kernel':kernel}]\n",
    "    grid_search = GridSearchCV(clf, param_grid=param_grid, cv=10)\n",
    "    grid_search.fit(data, target)\n",
    "\n",
    "    return grid_search\n",
    "grid_search = test_SVM(data_tfidf, target)\n",
    "print(grid_search.grid_scores_)\n",
    "grid_search.best_score_, grid_search.best_estimator_, grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "讠斤 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import loaddata\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import sklearn.feature_extraction.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages = loaddata.load_message()\n",
    "content = np.array([m[0] for m in messages])\n",
    "target = np.array([m[1] for m in messages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fantizi = loaddata.load_fantizi()\n",
    "content_after_fantizi = []\n",
    "processed = set()\n",
    "for i in content:\n",
    "    new_words=''\n",
    "    for k in i:\n",
    "        if k in fantizi:\n",
    "            new_words += fantizi[k]\n",
    "            processed.add((k, fantizi[k]))\n",
    "        else:\n",
    "            new_words += k\n",
    "    content_after_fantizi.append(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "讠斤 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "chaifenzi = loaddata.load_chaifenzi()\n",
    "content_after_chaifenzi = []\n",
    "found_chaifenzi = set()\n",
    "for line in content_after_fantizi:\n",
    "    result = line\n",
    "    for k,v in chaifenzi.items():\n",
    "        if k in line:\n",
    "            found_chaifenzi.add((k,v))\n",
    "            result = result.replace(k,v)\n",
    "    content_after_chaifenzi.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits_features = np.zeros((len(content),16))\n",
    "import re\n",
    "for i,line in enumerate(content):\n",
    "    for digits in re.findall(r'\\d+', line):\n",
    "        length = len(digits)\n",
    "        if 0 < length <= 15:\n",
    "            digits_features[i, length-1] += 1\n",
    "        elif length > 15:\n",
    "            digits_features[i, 15] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.873 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.873 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23734, 21948)\n"
     ]
    }
   ],
   "source": [
    "class TfidfVectorizer(sklearn.feature_extraction.text.TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        #analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        def analyzer(doc):\n",
    "            words = pseg.cut(doc)\n",
    "            new_doc = ''.join(w.word for w in words if w.flag != 'x')\n",
    "            words = jieba.cut(new_doc)\n",
    "            return words\n",
    "        return analyzer\n",
    "\n",
    "vec_tfidf = TfidfVectorizer(min_df=2,max_df=0.8)\n",
    "data_tfidf = vec_tfidf.fit_transform(content_after_chaifenzi)\n",
    "print(data_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_tfidf_digits = np.concatenate((data_tfidf.A, digits_features),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_SVM(data, target):\n",
    "    from sklearn.grid_search import GridSearchCV\n",
    "    from sklearn.svm import SVC\n",
    "    clf = SVC()\n",
    "\n",
    "    C = [2]\n",
    "    kernel = ['linear'] # 'poly',\n",
    "    param_grid = [{'C': C, 'kernel':kernel}]\n",
    "    grid_search = GridSearchCV(clf, param_grid=param_grid, cv=10)\n",
    "    grid_search.fit(data, target)\n",
    "\n",
    "    return grid_search\n",
    "grid_search = test_SVM(data_tfidf_digits, target)\n",
    "print(grid_search.grid_scores_)\n",
    "grid_search.best_score_, grid_search.best_estimator_, grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
